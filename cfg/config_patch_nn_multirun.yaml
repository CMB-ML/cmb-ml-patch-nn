# Do not name any yamls "default", "null", or "basic": these are reserved and will cause issues!
defaults:
  - local_system   : ${oc.env:CMB_ML_LOCAL_SYSTEM}
  - file_system    : common_fs
  - pipeline       : assembly_patch_nn
  - scenario       : scenario
  - splits         : all  # If you aren't getting as many results as expected, check n_infer_cap as well
  - model/patches  : healpatch
  - model/analysis : basic_analysis
  - model/patch_nn : basic_patch_nn
  - override hydra/job_logging: custom_log
  - _self_

# When creating multiple datasets, using interpolation like this may be easier:
# dataset_name       : ${scenario.map_fields}_${scenario.nside}_${splits.name}
dataset_name       : I_512_1450
working_dir        : "PatchNN_${model.patch_nn.scaling}_${junk_seed}/"
fig_model_name     : PatchNN
hydra:
  mode            : MULTIRUN
  sweeper:
    params:
      model.patch_nn.scaling: minmax, null
      junk_seed: 1, 2, 3, 4, 5
  run:
    dir            : Logs/${now:%Y-%m-%d-%H-%M-%S}
  verbose          : false

junk_seed: ???

# Settings below this point are used for interpolation.
# These are not to be used directly in the python
# They are picked up from here and MAY be used elsewhere in the yamls.
# In the scenario yaml
nside              : 512
detectors          : [30, 44, 70, 100, 143, 217, 353, 545, 857]
# detectors          : [100, 143, 217, 353]
map_fields         : "I"

# In the model yaml
num_epochs         : 100

# In the pipeline yamls
# For prediction, postprocessing, power spectra generation:
use_epochs           : [100] #, 60, 80, 100, 120]
# use_epochs           : [30, 50] #, 60, 80, 100, 120]
# For single simulation figures (maps & ps figures):
use_epochs_imgs      : ${use_epochs}
# For summary statistics:
use_epochs_map_stats : ${use_epochs}
# For summary statistics:
use_epochs_ps_stats  : ${use_epochs}
# Limit the number of simulations for which to generate figures
n_show_cap           : 1

# Limit the number of simulations on which to do inference and postprocessing
# In the splits yaml; null to run on all
n_infer_cap           : 2
run_inference_on      : valid  # Either "test" or "valid" ("train" should work, but is not recommended)
